{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "komma.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPgGwEZfEoKppc7NZMnpwnY",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeoLionel/komma/blob/main/komma.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWlodFcbpgVs"
      },
      "source": [
        "# Setup & dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQaED5WXHg72"
      },
      "source": [
        "# Download the training data to the colab local memory\n",
        "!gdown --id 1IAxYMM2dIdx3_HcwyQkfBSboBOzDrOKa\n",
        "!gdown --id 1TtcC9X6NBly4JS26E-1pAt9rkOHoXz0R"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAGAONP1TXIV"
      },
      "source": [
        "data_folder = '/content/'\n",
        "save_folder = '/content/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vd2QcS52SEED",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8df3a5dc-a3e4-44df-f6fa-d3db13e8c648"
      },
      "source": [
        "import time\n",
        "import pickle\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "torch.manual_seed(123)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using ', device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using  cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFg6PWS-pXLJ"
      },
      "source": [
        "# Dataset class\n",
        "\n",
        "When instantiated, the `SentenceDataV2` class loads the data from two provided files. One contains the initial word vectors (fastText), index-to-word and word-to-index dictionaries. The other, a numpy array with the input sequences and the output labels for the sequences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-U8ZzWZIUBV_"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class SentenceDataV2(Dataset):\n",
        "    def __init__(self, pickle_path, sentence_data_path, part = 'train', \n",
        "                 split = (4_500_000, 160_000)):\n",
        "\n",
        "        with open(pickle_path, 'rb') as f:\n",
        "            wordvecs, ix_to_word, word_to_ix = pickle.load(f)       \n",
        "            # wordvecs: numpy array of shape (num of vectors, dim of vectors)\n",
        "            # ix_to_word, word_to_ix: dictionaries \n",
        "\n",
        "        xy_pairs = np.load(sentence_data_path) \n",
        "        # numpy array of shape (n, 2, m)\n",
        "        # n: number of xy pairs, x: input sequence, y: sequence labels\n",
        "        # m: maximum sequence length (x & y are padded)\n",
        "\n",
        "        if part == 'train':\n",
        "            self.data = xy_pairs[:split[0]] \n",
        "        elif part == 'validation':\n",
        "            self.data = xy_pairs[-split[1]:] \n",
        "        else:\n",
        "            raise ValueError('Choose \"train\" or \"validation\" as \"part\" for the dataset')\n",
        "        \n",
        "        self.wordvecs = wordvecs \n",
        "        self.ix_to_word = ix_to_word  \n",
        "        self.word_to_ix = word_to_ix\n",
        "\n",
        "        self.output_class_weights = get_output_class_weights(xy_pairs)\n",
        "        self.output_class_ix = {'<pad>':      0,\n",
        "                                '<eos>':      1,\n",
        "                                '<comma>':    2,\n",
        "                                '<no_comma>': 3,\n",
        "                                }\n",
        "\n",
        "    def __getitem__(self, index):     \n",
        "        x = self.data[index][0]\n",
        "        y = self.data[index][1]\n",
        "        l = np.count_nonzero(y != self.word_to_ix['<pad>'])\n",
        "        return x[:l], y[:l], l\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "def get_output_class_weights(xy_pairs):\n",
        "    \"\"\"Calculate the inverse frequency of the three, non-pad output labels, \n",
        "    needed for the weighted cross-entropy loss. For a description of xy_pairs \n",
        "    check the class SentenceDataV2 above\"\"\"\n",
        "    ys = xy_pairs[:, 1, :]  \n",
        "    n = [0,0,0,0]\n",
        "    for i in range(4):\n",
        "       n[i] = np.count_nonzero(ys == i)\n",
        "    r = [1/x for x in n[1:]]\n",
        "    weights = [0] + [x/sum(r) for x in r] \n",
        "    return weights \n",
        "\n",
        "def num_commas (data_set: Dataset) -> int:\n",
        "    \"Count the number of sentences with a comma in data_set\"\n",
        "    comma_ix = data_set.output_class_ix['<comma>']\n",
        "    return sum([np.any(y == comma_ix) for _, y, _ in data_set])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMH6bFcLi73K"
      },
      "source": [
        "class SentenceData(Dataset):\n",
        "    \"\"\"Older and smaller data sets were saved to disk in a different format, \n",
        "    thus a second Dataset class for compatibility\"\"\"\n",
        "    def __init__(self, load_path, part = 'train', split = (500_000, 30_000)):\n",
        "\n",
        "        with open(load_path, 'rb') as f:\n",
        "            xy_pairs, wordvecs, ix_to_word, word_to_ix = pickle.load(f)\n",
        "\n",
        "        if part == 'train':\n",
        "            self.data = xy_pairs[:split[0]] # list of pairs of numpy arrays\n",
        "        elif part == 'validation':\n",
        "            self.data = xy_pairs[-split[1]:] # list of pairs of numpy arrays\n",
        "        elif part == 'just comma':\n",
        "            has_comma = lambda y: np.any(y == word_to_ix['<comma>'])\n",
        "            jc = [(x,y) for x,y in xy_pairs if has_comma(y)]\n",
        "            self.data = jc\n",
        "        else:\n",
        "            raise ValueError('Choose \"train\", \"validation\" or \"just comma\" as \"part\" for the dataset')\n",
        "        \n",
        "        self.wordvecs = wordvecs.astype('float32') # numpy array\n",
        "        self.ix_to_word = ix_to_word  # dict\n",
        "        self.word_to_ix = word_to_ix  # dict\n",
        "\n",
        "    def __getitem__(self, index):     \n",
        "        x, y = self.data[index]\n",
        "        return x, y, len(y)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIAZJRFck0Rv"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# The custom collate function to be used with the DataLoader class\n",
        "def pad_and_collate(triples):\n",
        "    xs, ys, ls = zip(*triples)\n",
        "    max_len = max(ls)\n",
        "    xs = [pad(x, max_len) for x in xs ]\n",
        "    ys = [pad(y, max_len) for y in ys ]\n",
        "    ls = list(ls)\n",
        "    return torch.tensor(xs), torch.tensor(ys), ls\n",
        "\n",
        "def pad(xs: np.ndarray, n: int) -> np.ndarray:\n",
        "    \"Pad a numpy array with zeros up to length n\"\n",
        "    m = len(xs)\n",
        "    if m < n:\n",
        "        return np.append(xs, [0]*(n-m)) \n",
        "    else:\n",
        "        return xs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrI8XRcUqM0W"
      },
      "source": [
        "# Comma Position Model\n",
        "A simple sequence tagging model using an bi-directional RNN. It takes in a batch of sequences (padded) and its lengths and returns the activatons after the last linear layer. For prediction or loss calculation, softmax still needs to be applied to the outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCOqonBEUmXG"
      },
      "source": [
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "class CommaPositionModel(nn.Module): \n",
        "    def __init__(self, wordvecs, shrink_emb_size, rnn_layers, rnn_hidden_size):\n",
        "  \n",
        "        super(CommaPositionModel, self).__init__()\n",
        "        self.out_classes = 4 \n",
        "        # the four prediction classes are:\n",
        "        # 0: dummy padding class (ignored in loss calculation)\n",
        "        # 1: eos after this sequence token\n",
        "        # 2: comma after this sequence token\n",
        "        # 3: no comma after this sequence token\n",
        "\n",
        "        # The weights for the classes depend on the training data, and are used\n",
        "        # for the cross entropy loss. Initialize accordingly!\n",
        "        self.class_weights = None  \n",
        "\n",
        "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(wordvecs),\n",
        "                                                      freeze = False,\n",
        "                                                      padding_idx = 0)\n",
        "        wordvec_size = wordvecs.shape[1]\n",
        "        if shrink_emb_size:\n",
        "            self.shrink_emb = nn.Linear(in_features = wordvec_size,\n",
        "                                        out_features = shrink_emb_size,\n",
        "                                        bias = True)\n",
        "        else: \n",
        "            self.shrink_emb = None\n",
        "        \n",
        "        rnn_in_size = shrink_emb_size if shrink_emb_size else wordvec_size\n",
        "        self.rnn = nn.GRU(num_layers = rnn_layers,\n",
        "                          input_size = rnn_in_size,\n",
        "                          hidden_size = rnn_hidden_size,\n",
        "                          bidirectional = True,\n",
        "                          batch_first = True)\n",
        "        \n",
        "        self.out_layer = nn.Linear(in_features = 2 * rnn_hidden_size,\n",
        "                                   out_features = self.out_classes,\n",
        "                                   bias = True) \n",
        "        \n",
        "        self.loss_fn = nn.CrossEntropyLoss(weight = self.class_weights,\n",
        "                                           ignore_index = 0)\n",
        "                                                    \n",
        "    def forward(self, xs, ls):\n",
        "        # xs is a batch of sequences, ls the lengths of the unpadded sequences\n",
        "        xs = self.embedding(xs)\n",
        "        if self.shrink_emb:\n",
        "            xs = self.shrink_emb(xs)\n",
        "        xs = pack_padded_sequence(xs, ls, batch_first = True,\n",
        "                                  enforce_sorted = False)\n",
        "            \n",
        "        # Calculate the output of the RNN. If h_0 is not provided,\n",
        "        # it defaults to zero. (Or h_0 and c_0 in case of an LSTM.)\n",
        "        xs, _ = self.rnn(xs)\n",
        "\n",
        "        # Unpack and drop the sequence lengths\n",
        "        xs, _ = pad_packed_sequence(xs, batch_first = True)\n",
        "\n",
        "        # Output layer\n",
        "        xs = self.out_layer(xs)\n",
        "        \n",
        "        return xs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qN9zbx4eWzZ1"
      },
      "source": [
        "def batch_loss (xs, ys, ls, model):\n",
        "    \"\"\"Calculate the cross entropy loss for a batch of sequences `xs` \n",
        "    with output labels `ys` and lengths `ls`\"\"\"\n",
        "    pred = model(xs, ls)\n",
        "    pred = pred.contiguous().view(-1, model.out_classes)\n",
        "    ys = ys.contiguous().view(-1)\n",
        "    return model.loss_fn(pred, ys)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6yn8mrqqjnH"
      },
      "source": [
        "# Validation loss, test accuracy and training loop\n",
        "\n",
        "Some helper functions to calculate the loss over the validation set and the accuracy of the model during training time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgYngWAEbTBe"
      },
      "source": [
        "PRED_BATCH_SIZE = 1_000\n",
        "\n",
        "def get_validation_loss(validation_set, model):\n",
        "    \"Calculate the loss over the validation set.\"\n",
        "    loader = DataLoader(validation_set, batch_size = PRED_BATCH_SIZE,\n",
        "                        shuffle = True, drop_last = True, \n",
        "                        collate_fn = pad_and_collate)       \n",
        "    with torch.no_grad():\n",
        "        valid_loss = 0\n",
        "        for batch_ix, (xs, ys, ls) in enumerate(loader, start = 1):\n",
        "            xs = xs.to(device)\n",
        "            ys = ys.to(device)\n",
        "            valid_loss += batch_loss(xs, ys, ls, model).item()\n",
        "    return valid_loss / batch_ix\n",
        "\n",
        "\n",
        "def get_accuracy(data_set, model):\n",
        "    \"Calculate the accuracy of the model on the training or validation set.\"\n",
        "    loader = DataLoader(data_set, batch_size = PRED_BATCH_SIZE, \n",
        "                        shuffle = False, collate_fn = pad_and_collate) \n",
        "    pad_ix = data_set.output_class_ix['<pad>']  \n",
        "    with torch.no_grad():\n",
        "        n = 0 # num of sequences with each word classified correct \n",
        "        k = 0 # num of sequences with comma in data_set\n",
        "        m = 0 # num of sequences with comma where each word is classified correct \n",
        "        for xs, ys, ls in loader:\n",
        "            xs = xs.to(device)\n",
        "            ys = ys.to(device)\n",
        "\n",
        "            pred = model(xs, ls)\n",
        "            pred = nn.functional.softmax(pred, dim = 2)\n",
        "            pred = torch.argmax(pred, dim = 2)\n",
        "            mask = (ys != pad_ix) \n",
        "            pred = pred * mask\n",
        "\n",
        "            predicted_right = torch.all(ys == pred, dim = 1)\n",
        "            n += torch.sum(predicted_right).item()\n",
        "\n",
        "            comma_ix = data_set.output_class_ix['<comma>']\n",
        "            with_comma = torch.any(ys == comma_ix, dim = 1)\n",
        "            k += torch.sum(with_comma).item()\n",
        "\n",
        "            m += torch.sum(predicted_right[with_comma]).item()\n",
        "\n",
        "        # Fraction of all sequences where each word is classified correct  \n",
        "        p1 = n / len(data_set) \n",
        "        # Fraction of sequences with comma where each word is classified correct \n",
        "        p2 = m / k\n",
        "        # Fraction of sequences without comma where each word is classified correct\n",
        "        p3 = (n - m ) / (len(data_set) - k + 1e-8)\n",
        "    return p1 * 100, p2 * 100, p3 * 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ymv08BoaR0Om"
      },
      "source": [
        "def print_run_summary(model, optimizer, dataset_name, tr_set, val_set, \n",
        "                      out_file=None):\n",
        "    \"Print a summary of the model, dataset and relevant global variables\"\n",
        "    if out_file:\n",
        "      def mprint(*s):\n",
        "        print(*s, file=out_file)\n",
        "    else:\n",
        "      def mprint(*s):\n",
        "        print(*s)\n",
        "    t = time.gmtime(time.time() + 2*60*60)\n",
        "    date_str = time.strftime(\"%Y-%m-%d %H:%M:%S\", t)\n",
        "    mprint(date_str,'\\n')\n",
        "    mprint('Dataset name:', dataset_name)\n",
        "    ls = len(tr_set), len(val_set)\n",
        "    mprint('Training / validation set size: {} / {}'.format(*ls))\n",
        "    # p = num_commas(training_set) / len(training_set) * 100\n",
        "    # mprint('Sentences with comma: {0: .2f}%'.format(p))\n",
        "    mprint('word vector size:', tr_set.wordvecs.shape[1])\n",
        "    mprint() \n",
        "\n",
        "    mprint('SHRINK_EMB_SIZE =', SHRINK_EMB_SIZE)\n",
        "    mprint('HIDDEN_SIZE =', HIDDEN_SIZE)\n",
        "    mprint('RNN_LAYERS =', RNN_LAYERS)\n",
        "    \n",
        "    mprint(model)\n",
        "    weights = [round(x, 2) for x in model.class_weights.numpy()]\n",
        "    mprint('class weights:', weights)\n",
        "    mprint()\n",
        "\n",
        "    mprint(optimizer)\n",
        "    mprint('BATCH_SIZE =', BATCH_SIZE)\n",
        "    mprint()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnlcHOwkboLZ"
      },
      "source": [
        "def train(model, optimizer, training_set, validation_set, epochs: int):  \n",
        "    \"\"\" Train the model for num of `epochs`. Loss, accuracy and model weights \n",
        "    are saved to global variables or to disk. \"\"\"\n",
        "    train_loader = DataLoader(training_set, batch_size = BATCH_SIZE, \n",
        "                              shuffle = True, collate_fn = pad_and_collate)  \n",
        "    print('start training')\n",
        "    tick = time.time()\n",
        "    loss_sum = 0\n",
        "    for ep_ix in range(1, epochs + 1, 1):\n",
        "        for batch_ix, (xs, ys, ls) in enumerate(train_loader, start = 1):\n",
        "            xs = xs.to(device)\n",
        "            ys = ys.to(device)\n",
        "            optimizer.zero_grad()    \n",
        "            loss = batch_loss(xs, ys, ls, model)\n",
        "            loss_sum += loss.detach().item()\n",
        "            # Backpropagation  \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if batch_ix % 100 == 0:\n",
        "                tr_loss = loss_sum / 100\n",
        "                loss_sum = 0\n",
        "                val_loss = get_validation_loss(validation_set, model)\n",
        "                TRAIN_LOSS.append(tr_loss)\n",
        "                VALID_LOSS.append(val_loss)\n",
        "                loss_info = tr_loss, val_loss, batch_ix*BATCH_SIZE, len(training_set)\n",
        "                print(\"loss: {0:.3f} {1:.3f} [{2}/{3}]\".format(*loss_info))\n",
        "\n",
        "        tock = time.time()\n",
        "        print('Epoch {0} finished after {1:.1f}s'.format(ep_ix, tock-tick))\n",
        "        print('Test accuracies (all / with comma / without comma):')  \n",
        "        ps_t = get_accuracy(training_set, model)\n",
        "        print( 'train: {0:.2f}% / {1:.2f}% / {2:.2f}%'.format(*ps_t) )   \n",
        "        ps_v = get_accuracy(validation_set, model)  \n",
        "        print( 'valid: {0:.2f}% / {1:.2f}% / {2:.2f}%'.format(*ps_v) ) \n",
        "        \n",
        "        if ep_ix == 1:\n",
        "            torch.save(model.state_dict(), SAVE_WEIGHTS_PATH )\n",
        "        if ep_ix > 1 and ps_v[1] > VALID_ACCUR[-1][1]:\n",
        "            torch.save(model.state_dict(), SAVE_WEIGHTS_PATH )\n",
        "        TRAIN_ACCUR.append(ps_t)\n",
        "        VALID_ACCUR.append(ps_v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpnnfPhmrCud"
      },
      "source": [
        "# Load data, initialize model & train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJAjZoFDMW17"
      },
      "source": [
        "dataset_name = 'dataset-v2-all-ml35-unk2-4660k'\n",
        "ds_path1 = data_folder + dataset_name + '-wv_dicts.pickle' \n",
        "ds_path2 = data_folder + dataset_name + '-xy_pairs.npy' \n",
        "\n",
        "ds_split = (4_500, 2_000) # Choose (4_500_000, 160_000) to use all data!\n",
        "\n",
        "training_set   = SentenceDataV2(ds_path1, ds_path2, part = 'train', split = ds_split)\n",
        "validation_set = SentenceDataV2(ds_path1, ds_path2, part = 'validation', split = ds_split)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECAPVQbHZF64"
      },
      "source": [
        "SHRINK_EMB_SIZE = None\n",
        "RNN_LAYERS = 3\n",
        "HIDDEN_SIZE = 1200\n",
        "\n",
        "LR = 5e-4\n",
        "BATCH_SIZE = 180\n",
        "\n",
        "model = CommaPositionModel(wordvecs = training_set.wordvecs, \n",
        "                           shrink_emb_size = SHRINK_EMB_SIZE,\n",
        "                           rnn_layers = RNN_LAYERS,\n",
        "                           rnn_hidden_size = HIDDEN_SIZE                         \n",
        "                           ).to(device)\n",
        "\n",
        "model.class_weights = torch.tensor(training_set.output_class_weights) \n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QB3Zz_b4iUaN"
      },
      "source": [
        "print_run_summary(model, optimizer, dataset_name, training_set, validation_set, out_file=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3M0iQEg1cdZS"
      },
      "source": [
        "run_name = 'test'\n",
        "\n",
        "epochs = 3\n",
        "\n",
        "with open(save_folder + run_name + '-info.txt', 'w') as f:\n",
        "    print_run_summary(model, optimizer, dataset_name, training_set, \n",
        "                      validation_set, out_file=f)\n",
        "\n",
        "TRAIN_LOSS = []\n",
        "VALID_LOSS = []\n",
        "TRAIN_ACCUR = []\n",
        "VALID_ACCUR = []\n",
        "\n",
        "SAVE_WEIGHTS_PATH = save_folder + run_name + '-model.weights'\n",
        "\n",
        "train(model, optimizer, training_set, validation_set, epochs)\n",
        "\n",
        "with open(save_folder + run_name + '-loss_accuracies.txt', 'w') as f:\n",
        "    out = TRAIN_LOSS, VALID_LOSS, TRAIN_ACCUR, VALID_ACCUR \n",
        "    json.dump(out, f, indent=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICQes5L5raUP"
      },
      "source": [
        "# Prediction and Evaluation code\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h16AH7XCt48D"
      },
      "source": [
        "# download pretrained weights to colab local storage\n",
        "!gdown --id 1-04Nl-w3EjJo_tILb4N9sqJR5Eh2fRVQ\n",
        "\n",
        "weights_path = save_folder + 'run-21-all-ml35-unk2-4460k-model.weights'\n",
        "saved_weights = torch.load(weights_path, map_location=device)\n",
        "model.load_state_dict(saved_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hw48vgDTHiL"
      },
      "source": [
        "def play(sentence: str, model: CommaPositionModel, word_to_ix) -> str:\n",
        "    \"Let the model predict the commas for a given input sentence\"\n",
        "    word_to_ix.default_factory = lambda: word_to_ix['<unk>']\n",
        "    words = sentence.split(' ')\n",
        "    seq = [word_to_ix[w] for w in words]\n",
        "    x = torch.tensor(seq).reshape(1,-1).to(device)\n",
        "    l = torch.tensor([len(seq)])\n",
        "    with torch.no_grad():\n",
        "        pred = model(x, l)\n",
        "        pred = nn.functional.softmax(pred, dim=2)\n",
        "        pred = torch.argmax(pred, dim=2)\n",
        "        pred = list(pred.reshape(-1).cpu().numpy())\n",
        "\n",
        "    # The indices of the output classes are: 3: no comma, 2: comma, 1: eos, 0: pad\n",
        "    # See documentation in SentenceDataV2, or CommaPositionModel class\n",
        "    d = {3: '', 2: ' ,', 1: ' <eos>'}\n",
        "    pseq = [d[ix] for ix in pred]   \n",
        "    out_str = ' '.join([a+b for a,b in zip(words,pseq)])\n",
        "    return out_str"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xUIh024nLNbF",
        "outputId": "bf71e43f-2077-44da-b4e5-5fc16569b835"
      },
      "source": [
        "play('Miriam schaut abends erst die Nachrichten weil sie sich informieren will später ihre Lieblingsserie', model, training_set.word_to_ix)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Miriam schaut abends erst die Nachrichten , weil sie sich informieren will , später ihre Lieblingsserie <eos>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyMasZY7ibb_"
      },
      "source": [
        "# try this:\n",
        "s = 'Das System erkennt die Sprache schnell und automatisch konvertiert die\\\n",
        " Wörter in die gewünschte Sprache und versucht die jeweiligen sprachlichen\\\n",
        " Nuancen und Ausdrücke hinzuzufügen'\n",
        "play(s, model, training_set.word_to_ix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHOLvMOnHlHT"
      },
      "source": [
        "def get_detail_error(data_set, model):\n",
        "    \"\"\"Calulate the error of the model separately by sentence length \n",
        "    and comma count of the sentence\"\"\"\n",
        "    loader = DataLoader(data_set, \n",
        "                        batch_size = PRED_BATCH_SIZE, \n",
        "                        shuffle = False, \n",
        "                        collate_fn = pad_and_collate)\n",
        "    \n",
        "    comma_ix = data_set.output_class_ix['<comma>']\n",
        "    pad_ix   = data_set.output_class_ix['<pad>']\n",
        "    max_len = 35 + 1 # maximum sequence length\n",
        "\n",
        "    # n[i,j]: number of sequences with lenght i and j commas in the data set\n",
        "    n = torch.zeros(max_len, max_len).to(device) \n",
        "\n",
        "    # m[i,j]: number of sequences with lenght i and j commas predicted wrong\n",
        "    m = torch.zeros(max_len, max_len).to(device)\n",
        "\n",
        "    with torch.no_grad(): \n",
        "        for xs, ys, ls in loader:\n",
        "            xs = xs.to(device)\n",
        "            ys = ys.to(device)\n",
        "            \n",
        "            pred = model(xs, ls)\n",
        "            pred = nn.functional.softmax(pred, dim = 2)\n",
        "            pred = torch.argmax(pred, dim = 2)\n",
        "            mask = (ys != pad_ix)\n",
        "            pred = pred * mask\n",
        "\n",
        "            predicted_wrong = torch.any(ys != pred, dim = 1)\n",
        "\n",
        "            ls = torch.tensor(ls).to(device)\n",
        "            cs = torch.count_nonzero(ys == comma_ix, dim = 1)\n",
        "          \n",
        "            # for i in range(len(pred)):\n",
        "            #     n[ls[i], cs[i]] += 1\n",
        "            #     if predicted_wrong[i]  : \n",
        "            #         m[ls[i], cs[i]] += 1\n",
        "            #\n",
        "            # The rest of the function does the same as the pevious four lines,\n",
        "            # just in a vectorized way for speed\n",
        "            # https://discuss.pytorch.org/t/convert-int-into-one-hot-format/507/4\n",
        "            # batch outer product https://discuss.pytorch.org/t/batch-outer-product/4025\n",
        "\n",
        "            r = torch.zeros(PRED_BATCH_SIZE, max_len).to(device).scatter_(1,ls.unsqueeze(1),1)\n",
        "            s = torch.zeros(PRED_BATCH_SIZE, max_len).to(device).scatter_(1,cs.unsqueeze(1),1)    \n",
        "            rxs = torch.bmm(r.unsqueeze(2), s.unsqueeze(1))\n",
        "            n += torch.sum(rxs, dim = 0)\n",
        "            \n",
        "            ls = ls[predicted_wrong]\n",
        "            cs = cs[predicted_wrong]\n",
        "            r = torch.zeros(len(ls), max_len).to(device).scatter_(1,ls.unsqueeze(1),1)\n",
        "            s = torch.zeros(len(cs), max_len).to(device).scatter_(1,cs.unsqueeze(1),1)\n",
        "            rxs = torch.bmm(r.unsqueeze(2), s.unsqueeze(1))\n",
        "            m += torch.sum(rxs, dim = 0)\n",
        "\n",
        "    p = m / n * 100\n",
        "    return p, n "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xj-gRc9T6ywv"
      },
      "source": [
        "ds_split = (4_500, 160_000) # Ensure to use all of the validation set now\n",
        "validation_set = SentenceDataV2(ds_path1, ds_path2, part = 'validation', split = ds_split)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwrIvWnGuVNL",
        "outputId": "286c067f-e65f-460c-a049-640b1e117dc3"
      },
      "source": [
        "p, n = get_detail_error(validation_set, model)\n",
        "# p[i, j] gives the error rate on sentences of length i with j commas\n",
        "# n[i, j] gives the amount of sentences of length i with j commas in the validation set\n",
        "p[1:, :7]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[     nan,      nan,      nan,      nan,      nan,      nan,      nan],\n",
              "        [  0.0000,  50.0000,      nan,      nan,      nan,      nan,      nan],\n",
              "        [  0.6468,  47.6190,  42.8571,      nan,      nan,      nan,      nan],\n",
              "        [  1.0688,  38.6667,  75.0000,   0.0000,      nan,      nan,      nan],\n",
              "        [  1.0198,  37.5000,  57.1429, 100.0000,      nan,      nan,      nan],\n",
              "        [  1.3619,  25.1429,  65.0000,  50.0000,      nan,      nan,      nan],\n",
              "        [  1.7312,  21.8009,  57.1429,   0.0000,   0.0000,   0.0000,      nan],\n",
              "        [  2.3057,  18.9838,  42.2414,  46.1538,      nan, 100.0000,      nan],\n",
              "        [  2.7925,  16.0595,  38.0711,  50.0000,   0.0000, 100.0000,      nan],\n",
              "        [  3.4471,  13.9373,  32.6154,  44.0000,  33.3333,   0.0000,   0.0000],\n",
              "        [  4.3956,  12.1481,  32.2581,  46.5116,  50.0000,      nan,      nan],\n",
              "        [  5.4476,  11.4348,  26.6212,  50.0000,  71.4286,   0.0000,   0.0000],\n",
              "        [  5.7821,  11.7992,  26.5278,  41.9355,  26.6667,  25.0000, 100.0000],\n",
              "        [  6.9741,  11.5814,  27.8986,  41.5254,  36.8421,  50.0000,  50.0000],\n",
              "        [  8.1775,  12.6308,  25.4469,  41.8919,  33.3333,  14.2857,      nan],\n",
              "        [  8.6634,  11.5656,  22.7761,  42.7746,  50.0000,  16.6667, 100.0000],\n",
              "        [  9.7497,  11.4580,  23.5996,  47.3958,  58.8235,  44.4444,   0.0000],\n",
              "        [ 11.0040,  12.7538,  23.9782,  40.0966,  40.0000,  71.4286,  25.0000],\n",
              "        [ 11.8631,  13.4709,  23.5568,  40.3846,  51.2195,  69.2308,  33.3333],\n",
              "        [ 14.3158,  13.7506,  24.1573,  44.2308,  62.7907,  46.6667,  75.0000],\n",
              "        [ 13.7339,  13.3626,  21.5075,  39.3502,  57.1429,  63.6364,   0.0000],\n",
              "        [ 15.3285,  13.3569,  25.0287,  35.2273,  47.1698,  69.2308,      nan],\n",
              "        [ 15.0943,  16.0334,  23.2617,  38.4342,  47.7612,  36.3636,   0.0000],\n",
              "        [ 16.3399,  16.6667,  23.0000,  32.9114,  37.0968,  61.5385,  50.0000],\n",
              "        [ 21.3333,  16.7160,  23.1959,  39.4052,  56.5789,  38.4615,  66.6667],\n",
              "        [ 18.5897,  15.3285,  23.1225,  38.4956,  45.3333, 100.0000,  80.0000],\n",
              "        [ 22.6804,  19.1214,  25.7908,  42.0213,  54.5455,  61.5385,  50.0000],\n",
              "        [ 22.0779,  22.5904,  19.0751,  30.4878,  54.7170,  61.5385,  25.0000],\n",
              "        [ 29.5455,  19.7628,  25.2874,  34.6405,  52.8302,  62.5000, 100.0000],\n",
              "        [ 16.1290,  23.6025,  27.4112,  34.1085,  48.6487,  75.0000,      nan],\n",
              "        [ 39.2857,  20.0000,  28.7234,  38.8889,  52.3810,      nan,      nan],\n",
              "        [ 16.6667,  24.7312,  32.1678,  37.0370,      nan,      nan,      nan],\n",
              "        [ 36.3636,  24.0741,  23.4043,      nan,      nan,      nan,      nan],\n",
              "        [ 37.5000,  33.3333,      nan,      nan,      nan,      nan,      nan],\n",
              "        [ 42.8571,      nan,      nan,      nan,      nan,      nan,      nan]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzYUaSzQsbVG"
      },
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "def get_precision_recall(data_set, model):\n",
        "    \"Calculate precision, recall and f1 for the model on a data set.\"\n",
        "    loader = DataLoader(data_set, batch_size = PRED_BATCH_SIZE, \n",
        "                        shuffle = False, drop_last = True,\n",
        "                        collate_fn = pad_and_collate)\n",
        "    num_classes = 3\n",
        "    pad_ix = data_set.output_class_ix['<pad>'] \n",
        "\n",
        "    precision = np.zeros(num_classes)\n",
        "    recall = np.zeros(num_classes)\n",
        "    f1 = np.zeros(num_classes)\n",
        "\n",
        "    with torch.no_grad(): \n",
        "        for i, (xs, ys, ls) in enumerate(loader, start = 1):\n",
        "            xs = xs.to(device)\n",
        "            ys = ys.to(device)\n",
        "            \n",
        "            pred = model(xs, ls)\n",
        "            pred = nn.functional.softmax(pred, dim = 2)\n",
        "            pred = torch.argmax(pred, dim = 2)\n",
        "\n",
        "            # get rid of padding tokens, this reshapes the tensors to 1D\n",
        "            mask = (ys != 0)\n",
        "            pred = pred[mask].cpu().numpy()\n",
        "            ys = ys[mask].cpu().numpy()\n",
        "\n",
        "            pr, re, f, _ = precision_recall_fscore_support(ys, pred)\n",
        "            precision += pr\n",
        "            recall += re\n",
        "            f1 += f\n",
        "\n",
        "    return precision / i, recall / i, f1 / i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aIKScAU9wZ3",
        "outputId": "52515082-4067-4689-85b6-8a0c5169c8ba"
      },
      "source": [
        "# pecision, recall and f1 for the three classes: 'eos', 'comma', 'word'\n",
        "get_precision_recall(validation_set, model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([1.        , 0.89568644, 0.99417314]),\n",
              " array([1.        , 0.88764191, 0.99463348]),\n",
              " array([1.        , 0.89155284, 0.99440299]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    }
  ]
}